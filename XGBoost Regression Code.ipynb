{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is inspired by: \n",
    "\n",
    "- https://www.kaggle.com/code/carlmcbrideellis/an-introduction-to-xgboost-regression\n",
    "- https://www.kaggle.com/code/dansbecker/xgboost/notebook\n",
    "\n",
    "In this assignment we will apply XGBoost Regression techniques to predict house prices, based on the famous Kaggle Dataset https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7842527-0531-44fa-8ce0-ae4cc3cfd0d7",
    "_uuid": "365f3e96-c7a9-41cf-9c5f-76dbfd46168c",
    "execution": {
     "iopub.execute_input": "2022-10-17T08:49:03.875064Z",
     "iopub.status.busy": "2022-10-17T08:49:03.874451Z",
     "iopub.status.idle": "2022-10-17T08:49:03.951957Z",
     "shell.execute_reply": "2022-10-17T08:49:03.950940Z",
     "shell.execute_reply.started": "2022-10-17T08:49:03.874981Z"
    }
   },
   "outputs": [],
   "source": [
    "#=========================================================================\n",
    "# load up the libraries\n",
    "#=========================================================================\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "# Change settings for viewing records \n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "#=========================================================================\n",
    "# read in the data\n",
    "#=========================================================================\n",
    "train_data = pd.read_csv('train.csv',index_col=0)\n",
    "test_data  = pd.read_csv('test.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Exploration, Selection and Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore the data \n",
    "\n",
    "# print(train_data.head())\n",
    "# print(train_data.info())\n",
    "# print(train_data.describe())\n",
    "\n",
    "# Divide into train and validate (80 train and 20 validate)\n",
    "train_df, validate_df = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Drop columns with missing values > 10%\n",
    "missing_threshold = 0.1 * len(train_df)\n",
    "train_df = train_df.dropna(thresh=missing_threshold, axis=1)\n",
    "validate_df = validate_df[train_df.columns]\n",
    "\n",
    "\n",
    "# Retain columns that are statistically significant (T-test for categorical columns and correlation for numerical columns)\n",
    "# T-test for categorical columns\n",
    "categorical_cols = train_df.select_dtypes(include='object').columns\n",
    "significant_categorical_cols = []\n",
    "for col in categorical_cols:\n",
    "    t_stat, p_value = stats.ttest_ind(train_df[train_df['SalePrice'] == 0][col], train_df[train_df['SalePrice'] == 1][col])\n",
    "    if p_value < 0.05:\n",
    "        significant_categorical_cols.append(col)\n",
    "\n",
    "\n",
    "\n",
    "# Correlation for numerical columns\n",
    "numerical_cols = train_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "corr_matrix = train_df[numerical_cols].corr()\n",
    "significant_numerical_cols = [col for col in numerical_cols if abs(corr_matrix['SalePrice'][col]) > 0.1]\n",
    "\n",
    "significant_cols = significant_categorical_cols + significant_numerical_cols\n",
    "\n",
    "\n",
    "train_df = train_df[significant_cols]\n",
    "validate_df = validate_df[significant_cols]\n",
    "\n",
    "# # Separate features and target variable\n",
    "X_train = train_df.drop('SalePrice', axis=1)\n",
    "y_train = train_df[['SalePrice']]\n",
    "X_validate = validate_df.drop('SalePrice', axis=1)\n",
    "y_validate = validate_df[['SalePrice']]\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = XGBRegressor()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the validate set\n",
    "y_pred = xgb_model.predict(X_validate)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_validate, y_pred)\n",
    "print(f'Mean Squared Error on Validate Set: {mse}')\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(XGBRegressor(), param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Retrain the model on the entire train dataset (train + validate) with the best hyperparameters\n",
    "best_xgb_model = XGBRegressor(**best_params)\n",
    "\n",
    "X_combined = pd.concat([X_train, X_validate], ignore_index=True)\n",
    "y_combined = pd.concat([y_train, y_validate], ignore_index=True)\n",
    "best_xgb_model.fit(X_combined, y_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Test Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[test_data.columns[test_data.columns.isin(X_combined.columns)]]\n",
    "test_data.info()\n",
    "test_data['BsmtFinSF1'].astype('float64')\n",
    "test_data['BsmtUnfSF'].astype('float64')\n",
    "test_data['TotalBsmtSF'].astype('float64')\n",
    "test_data['BsmtFullBath'].astype('float64')\n",
    "y_test = best_xgb_model.predict(test_data)\n",
    "y_test = pd.DataFrame(y_test)\n",
    "y_test.columns = ['Predicted_Sales_Price']\n",
    "y_test.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
